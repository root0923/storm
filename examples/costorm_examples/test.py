from knowledge_storm.collaborative_storm.engine import CollaborativeStormLMConfigs, RunnerArgument, CoStormRunner
from knowledge_storm.lm import DeepSeekModel
from knowledge_storm.logging_wrapper import LoggingWrapper
from knowledge_storm.rm import SerperRM
from knowledge_storm.collaborative_storm.modules.callback import (
    LocalConsolePrintCallBackHandler,
)
from knowledge_storm.encoder import Encoder
from config import MODEL_CONFIG, SEARCH_CONFIG, EMBEDDING_CONFIG
import os
import json
import re
import logging
import sys
from datetime import datetime

class RealTimeFileHandler(logging.FileHandler):
    def __init__(self, filename, mode='a', encoding='utf-8', delay=False):
        super().__init__(filename, mode, encoding, delay)
        
    def emit(self, record):
        try:
            super().emit(record)
            self.flush() 
        except Exception as e:
            print(f"æ—¥å¿—å†™å…¥é”™è¯¯: {e}")

for handler in logging.root.handlers[:]:
    logging.root.removeHandler(handler)

def dual_print(message):
    """åŒæ—¶æ‰“å°åˆ°æ§åˆ¶å°å’Œè®°å½•åˆ°æ—¥å¿—"""
    original_print(message)
    logging.info(message)
    for handler in logging.getLogger().handlers:
        handler.flush()

original_print = print
print = dual_print

# ============ encoder configurations ============ 
os.environ["ENCODER_API_TYPE"] = "openai"

# Co-STORM adopts the same multi LM system paradigm as STORM 
lm_config: CollaborativeStormLMConfigs = CollaborativeStormLMConfigs()
kwargs = {
    "api_key": MODEL_CONFIG["api_key"],
    "api_base": MODEL_CONFIG["api_base"],
    "temperature": 1.0,
    "top_p": 0.9,
}
model_name = MODEL_CONFIG["model_name"]

question_answering_lm = DeepSeekModel(model=model_name, max_tokens=3000, **kwargs )
discourse_manage_lm = DeepSeekModel(model=model_name, max_tokens=1500, **kwargs )
utterance_polishing_lm = DeepSeekModel(model=model_name, max_tokens=3000, **kwargs )
warmstart_outline_gen_lm = DeepSeekModel(model=model_name, max_tokens=500, **kwargs )
question_asking_lm = DeepSeekModel(model=model_name, max_tokens=1000, **kwargs )
knowledge_base_lm = DeepSeekModel(model=model_name, max_tokens=1000, **kwargs )

lm_config.set_question_answering_lm(question_answering_lm)
lm_config.set_discourse_manage_lm(discourse_manage_lm)
lm_config.set_utterance_polishing_lm(utterance_polishing_lm)
lm_config.set_warmstart_outline_gen_lm(warmstart_outline_gen_lm)
lm_config.set_question_asking_lm(question_asking_lm)
lm_config.set_knowledge_base_lm(knowledge_base_lm)


# Check out the Co-STORM's RunnerArguments class for more configurations.
topic = input("è¯·è¾“å…¥è¯é¢˜: ")

# é‡æ–°é…ç½®å®Œæ•´çš„æ—¥å¿—è®°å½•ç³»ç»Ÿ
file_handler = RealTimeFileHandler(f'../../log/{topic}.log', mode='a', encoding='utf-8')
file_handler.setLevel(logging.INFO)
file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))

console_handler = logging.StreamHandler(sys.stdout)
console_handler.setLevel(logging.INFO)
console_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))

logging.root.setLevel(logging.INFO)
logging.root.addHandler(file_handler)
logging.root.addHandler(console_handler)

logging.info(f"ç”¨æˆ·è¾“å…¥è¯é¢˜: {topic}")

runner_argument = RunnerArgument(
    topic=topic,
    warmstart_max_num_experts=3,  # warm starté˜¶æ®µçš„ä¸“å®¶æ•°é‡ï¼ˆç›¸å½“äºè§†è§’ä¸ªæ•°ï¼‰
    max_num_round_table_experts=3,  # åœ†æ¡Œè®¨è®ºä¸­çš„æ´»è·ƒä¸“å®¶æ•°é‡
    warmstart_max_turn_per_experts=1,  # æ¯ä¸ªä¸“å®¶åœ¨warm starté˜¶æ®µçš„æœ€å¤§è½®æ•°
    moderator_override_N_consecutive_answering_turn=3,  # ä¸»æŒäººå¹²é¢„å‰çš„è¿ç»­å›ç­”è½®æ•°
)
logging_wrapper = LoggingWrapper(lm_config)
callback_handler = LocalConsolePrintCallBackHandler()
serper_rm = SerperRM(
                serper_search_api_key=SEARCH_CONFIG["serper_api_key"],
                query_params={"autocorrect": True, "num": 10, "page": 1},
            )

# åˆ›å»ºå¸¦æœ‰embeddingé…ç½®çš„encoder
encoder = Encoder(embedding_config=EMBEDDING_CONFIG)

costorm_runner = CoStormRunner(lm_config=lm_config,
                               runner_argument=runner_argument,
                               logging_wrapper=logging_wrapper,
                               rm=serper_rm,
                               callback_handler=callback_handler,
                               encoder=encoder
                               )

def print_kb_structure(costorm_runner, step_info=""):
    """æ‰“å°çŸ¥è¯†åº“çš„æ ‘å½¢ç»“æ„å’Œå½“å‰ä¸“å®¶åˆ—è¡¨"""
    print(f"\n{'='*60}")
    print(f"Knowledge Base ç»“æ„ {step_info}")
    print(f"{'='*60}")
    
    print(f"\nçŸ¥è¯†åº“ç»“æ„:")
    kb_structure = costorm_runner.knowledge_base.get_node_hierarchy_string(
        include_indent=True,
        include_hash_tag=True,
        include_node_content_count=True
    )
    
    if kb_structure.strip():
        print(kb_structure)
    else:
        print("çŸ¥è¯†åº“ç›®å‰ä¸ºç©º")
    
    print(f"{'='*60}\n")

print("ğŸ”¥ warm start...")
# Warm start the system to build shared conceptual space between Co-STORM and users
costorm_runner.warm_start()

# æ‰“å°warm starté˜¶æ®µçš„conversationå†…å®¹
print("\n Warm Starté˜¶æ®µçš„å¯¹è¯å†…å®¹:")
for turn in costorm_runner.warmstart_conv_archive:
    print(f"**{turn.role}**\n{turn.utterance_type}:{turn.utterance}\n")

# æ‰“å°warm startåçš„çŸ¥è¯†åº“ç»“æ„
print_kb_structure(costorm_runner, "(Warm Startå)")

print("ğŸ’¬ start conversation...")
# Step through the collaborative discourse 
# Run either of the code snippets below in any order, as many times as you'd like
# To observe the conversation:
successful_turns = 0
step_count = 0

while True:
    print("è¯·é€‰æ‹©ä¸‹ä¸€æ­¥æ“ä½œï¼š")
    print("1: ç³»ç»Ÿç”Ÿæˆè¯è¯­ï¼›2: ç”¨æˆ·è¾“å…¥è¯è¯­ï¼›3: ç»ˆæ­¢ç”Ÿæˆè¯è¯­ï¼Œç”Ÿæˆreport")
    choice = input("è¯·è¾“å…¥é€‰é¡¹ï¼ˆ1/2/3ï¼‰: ")
    logging.info(f"ç”¨æˆ·é€‰æ‹©: {choice}")
    
    if choice == "1":
        step_count += 1
        conv_turn = costorm_runner.step()
        while conv_turn is None:
            conv_turn = costorm_runner.step()
        print(f"**{conv_turn.role}**\n{conv_turn.utterance_type}:{conv_turn.utterance}\n")
        if conv_turn.role == "Moderator":
            print_kb_structure(costorm_runner, f"(Step {step_count}å)")
        elif conv_turn.role == "General Knowledge Provider":
            print("å½“å‰ä¸“å®¶åˆ—è¡¨:")
            if hasattr(costorm_runner, 'discourse_manager') and hasattr(costorm_runner.discourse_manager, 'experts'):
                experts = costorm_runner.discourse_manager.experts
                if experts:
                    for expert in experts:
                        print(f"   {expert.role_name}")
                else:
                    print("  æš‚æ— ä¸“å®¶")
            else:
                print("  ä¸“å®¶ä¿¡æ¯ä¸å¯ç”¨")
        
    elif choice == "2":
        step_count += 1
        user_utterance = input("è¯·è¾“å…¥ç”¨æˆ·è¯è¯­: ")
        logging.info(f"ç”¨æˆ·è¾“å…¥è¯è¯­: {user_utterance}")
        costorm_runner.step(user_utterance=user_utterance)
        # ç”¨æˆ·è¾“å…¥åè‡ªåŠ¨æ‰§è¡Œä¸€æ¬¡ç³»ç»Ÿç”Ÿæˆè¯è¯­
        step_count += 1
        conv_turn = costorm_runner.step()
        while conv_turn is None:
            print("è¯­å¥ä¸ºç©ºï¼Œé‡æ–°ç”Ÿæˆ")
            conv_turn = costorm_runner.step()
        print(f"**{conv_turn.role}**\n{conv_turn.utterance_type}:{conv_turn.utterance}\n")
        if conv_turn.role == "Moderator":
            print_kb_structure(costorm_runner, f"(Step {step_count}å)")
        elif conv_turn.role == "General Knowledge Provider":
            print("å½“å‰ä¸“å®¶åˆ—è¡¨:")
            if hasattr(costorm_runner, 'discourse_manager') and hasattr(costorm_runner.discourse_manager, 'experts'):
                experts = costorm_runner.discourse_manager.experts
                if experts:
                    for expert in experts:
                        print(f"   {expert.role_name}")
                else:
                    print("  æš‚æ— ä¸“å®¶")
            else:
                print("  ä¸“å®¶ä¿¡æ¯ä¸å¯ç”¨")
        
    elif choice == "3":
        break
    else:
        print("æ— æ•ˆé€‰é¡¹ï¼Œè¯·é‡æ–°è¾“å…¥ã€‚")


# åœ¨ç”ŸæˆæŠ¥å‘Šå‰æ‰“å°é‡ç»„å‰çš„ç»“æ„
print("ğŸ“„ generate report...")
print_kb_structure(costorm_runner, "(é‡ç»„å‰)")

costorm_runner.knowledge_base.reorganize()

# æ‰“å°é‡ç»„åçš„ç»“æ„
print_kb_structure(costorm_runner, "(é‡ç»„å)")

article = costorm_runner.generate_report()

print(' >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ')
print(article)
print(' >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ')
print("âœ… report has been generated")