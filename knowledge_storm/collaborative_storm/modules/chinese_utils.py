"""
ä¸­æ–‡å¤„ç†å·¥å…·é›†
åŒ…å«ä¸­æ–‡è¾“å‡ºæ¸…ç†ã€æ–‡æœ¬æ ‡å‡†åŒ–ã€ç»“æ„è§£æç­‰åŠŸèƒ½
åˆå¹¶äº† chinese_output_processor.py å’Œ chinese_text_utils.py çš„æ‰€æœ‰åŠŸèƒ½
"""

import re
from typing import Any, Dict, List, Tuple


# ============================================================================
# ä¸­æ–‡è¾“å‡ºæ¸…ç†åŠŸèƒ½ (åŸ chinese_output_processor.py)
# ============================================================================

def clean_chinese_output(text: str, role_context: str = "") -> str:
    """
    æ¸…ç†æ¨¡å‹è¾“å‡ºï¼Œå¼ºåˆ¶è½¬æ¢ä¸ºçº¯ä¸­æ–‡å†…å®¹
    
    Args:
        text (str): åŸå§‹æ¨¡å‹è¾“å‡º
        role_context (str): è§’è‰²ä¸Šä¸‹æ–‡ï¼Œå¦‚"Moderator"ã€"Expert"ç­‰ï¼Œç”¨äºè°ƒæ•´æ¸…ç†ç­–ç•¥
        
    Returns:
        str: æ¸…ç†åçš„ä¸­æ–‡è¾“å‡º
    """
    if not text:
        return text
        
    # 1. ç§»é™¤ <think> æ ‡ç­¾åŠå…¶å†…å®¹
    text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
    text = re.sub(r'<think>', '', text)
    text = re.sub(r'</think>', '', text)
    
    # 2. ç§»é™¤ã€æ³¨ï¼š...ã€‘æ ¼å¼çš„æ³¨é‡Šå†…å®¹
    text = re.sub(r'ã€æ³¨ï¼š.*?ã€‘', '', text, flags=re.DOTALL)
    text = re.sub(r'\[æ³¨ï¼š.*?\]', '', text, flags=re.DOTALL)
    
    # 3. ç§»é™¤å¸¸è§çš„è‹±æ–‡æ€è€ƒå¼€å¤´
    english_thinking_patterns = [
        r'^(Okay|Alright|Let me|I need to|First|So|Well),.*?(?=\n\n|\n[A-Z]|\n\d+\.)',
        r'^The (user|question|topic).*?(?=\n\n|\n[A-Z]|\n\d+\.)',
        r'^Looking at.*?(?=\n\n|\n[A-Z]|\n\d+\.)',
        r'^Based on.*?(?=\n\n|\n[A-Z]|\n\d+\.)',
    ]
    
    for pattern in english_thinking_patterns:
        text = re.sub(pattern, '', text, flags=re.MULTILINE | re.DOTALL)
    
    # 4. ğŸ”´ æ£€æµ‹å¹¶ç§»é™¤å…ƒè®¤çŸ¥æš´éœ²ï¼ˆAIå†…éƒ¨æ€è€ƒè¿‡ç¨‹ï¼‰
    # ğŸŸ¢ ä¸ºModeratorè§’è‰²æä¾›æ›´å®½æ¾çš„æ¸…ç†ç­–ç•¥
    if role_context.lower() == "moderator":
        # Moderatorçš„å…ƒè®¤çŸ¥æ¨¡å¼æ›´ä¿å®ˆï¼Œåªåˆ é™¤æ˜æ˜¾çš„æ€è€ƒè¿‡ç¨‹
        metacognitive_patterns = [
            r'å¥½çš„ï¼Œæˆ‘(ç°åœ¨)?éœ€è¦å¤„ç†ç”¨æˆ·(æä¾›çš„)?.*?(?=\n\n|$)',
            r'é¦–å…ˆï¼Œæˆ‘(éœ€è¦)?ä»”ç»†é˜…è¯»ç”¨æˆ·.*?(?=\n\n|$)', 
            r'æ¥ä¸‹æ¥ï¼Œæˆ‘(éœ€è¦)?ç¡®å®š.*?(?=\n\n|$)',
            r'æˆ‘(éœ€è¦)?æ£€æŸ¥æ˜¯å¦.*?(?=\n\n|$)',
            r'ç°åœ¨ï¼Œæˆ‘(éœ€è¦)?å°†è¿™äº›æ€è€ƒè½¬åŒ–ä¸º.*?(?=\n\n|$)',
            r'æ ¹æ®ç”¨æˆ·.*?(?=\n\n|$)',
        ]
    else:
        # æ™®é€šä¸“å®¶çš„å®Œæ•´å…ƒè®¤çŸ¥æ¸…ç†
        metacognitive_patterns = [
            r'å¥½çš„ï¼Œæˆ‘(ç°åœ¨)?éœ€è¦å¤„ç†ç”¨æˆ·(æä¾›çš„)?.*?(?=\n\n|$)',
            r'é¦–å…ˆï¼Œæˆ‘(éœ€è¦)?ä»”ç»†é˜…è¯»ç”¨æˆ·.*?(?=\n\n|$)', 
            r'æ¥ä¸‹æ¥ï¼Œæˆ‘(éœ€è¦)?ç¡®å®š.*?(?=\n\n|$)',
            r'ç„¶åï¼Œæˆ‘(éœ€è¦)?è€ƒè™‘.*?(?=\n\n|$)',
            r'æœ€åï¼Œæˆ‘?(éœ€è¦)?ç¡®ä¿.*?(?=\n\n|$)',
            r'æˆ‘(éœ€è¦)?æ£€æŸ¥æ˜¯å¦.*?(?=\n\n|$)',
            r'ç°åœ¨ï¼Œæˆ‘(éœ€è¦)?å°†è¿™äº›æ€è€ƒè½¬åŒ–ä¸º.*?(?=\n\n|$)',
            r'ç”¨æˆ·(å¯èƒ½å¸Œæœ›|è¦æ±‚|æåˆ°).*?(?=\n\n|$)',
            r'æ ¹æ®ç”¨æˆ·.*?(?=\n\n|$)',
            r'è¿™äº›ä¿¡æ¯éœ€è¦æ•´åˆ.*?(?=\n\n|$)',
            r'å¯èƒ½çš„ç»“æ„æ˜¯.*?(?=\n\n|$)',
            r'éœ€è¦ç¡®è®¤ç”¨æˆ·.*?(?=\n\n|$)',
            r'ä½†ç”¨æˆ·.*?(?=\n\n|$)',
            r'ä¸è¿‡ç”¨æˆ·.*?(?=\n\n|$)',
            r'æ­¤å¤–ï¼Œå¿…é¡»ä½¿ç”¨æ­£ç¡®çš„è¡Œå†…å¼•ç”¨.*?(?=\n\n|$)',
            r'æˆ‘éœ€è¦æ£€æŸ¥æ˜¯å¦æœ‰é—æ¼.*?(?=\n\n|$)',
            r'éœ€è¦å°†è¿™äº›ä¿¡æ¯æµ“ç¼©æˆ.*?(?=\n\n|$)',
            r'æ£€æŸ¥æ˜¯å¦æœ‰é—æ¼çš„ä¿¡æ¯ç‚¹.*?(?=\n\n|$)',
            r'ç¡®ä¿æ¯ä¸ªå¼•ç”¨éƒ½æ­£ç¡®å¯¹åº”.*?(?=\n\n|$)',
            r'æœ€ç»ˆï¼Œç»„ç»‡è¯­è¨€.*?(?=\n\n|$)',
            r'å¯èƒ½åˆ†ä¸ºå‡ ä¸ªä¸»è¦.*?(?=\n\n|$)',
            r'æˆ‘éœ€è¦å…ˆç¡®å®š.*?(?=\n\n|$)',
            r'ç„¶åï¼Œæ³¨æ„è¡Œå†…å¼•ç”¨.*?(?=\n\n|$)',
            r'éœ€è¦ç¡®ä¿è¯­è¨€æµç•….*?(?=\n\n|$)',
        ]
    
    for pattern in metacognitive_patterns:
        text = re.sub(pattern, '', text, flags=re.MULTILINE | re.DOTALL)
    
    # 5. æ™ºèƒ½å¤„ç†ä¸­æ–‡æ€è€ƒè¿‡ç¨‹ï¼šä¿ç•™æœ‰å®è´¨å†…å®¹å’Œå¼•ç”¨çš„å¥å­
    lines = text.split('\n')
    useful_lines = []
    
    # æ›´ç²¾ç¡®çš„æ€è€ƒæŒ‡ç¤ºå™¨
    thinking_indicators = [
        'å¥½çš„ï¼Œæˆ‘ç°åœ¨éœ€è¦å¤„ç†',
        'é¦–å…ˆï¼Œæˆ‘éœ€è¦ä»”ç»†é˜…è¯»',
        'æ¥ä¸‹æ¥ï¼Œæˆ‘éœ€è¦ç¡®å®š',
        'ç„¶åï¼Œéœ€è¦ç»„ç»‡è¿™äº›ä¿¡æ¯',
        'æœ€åï¼Œè¦ç¡®ä¿è¯­è¨€æµç•…',
        'ç°åœ¨ï¼Œæˆ‘éœ€è¦å°†è¿™äº›æ€è€ƒè½¬åŒ–ä¸º',
        'å¯èƒ½çš„ç»“æ„æ˜¯',
        'éœ€è¦ç¡®ä¿å¼•ç”¨æ­£ç¡®',
        'åœ¨ç»„ç»‡è¯­è¨€æ—¶',
        'æœ€åï¼Œç¡®ä¿å†…å®¹æµç•…',
        'éœ€è¦æ³¨æ„ä¸è¦é‡å¤ä¹‹å‰çš„å†…å®¹',
        'ç°åœ¨ï¼Œæˆ‘éœ€è¦å°†è¿™äº›æ€è€ƒ',
        # ğŸ”´ æ–°å¢ï¼šå¤„ç†æµ‹è¯•ç”¨ä¾‹ä¸­çš„æ€è€ƒæ¨¡å¼
        'éœ€è¦å°†è¿™äº›ä¿¡æ¯æµ“ç¼©æˆ',
        'æ£€æŸ¥æ˜¯å¦æœ‰é—æ¼çš„ä¿¡æ¯ç‚¹',
        'ç¡®ä¿æ¯ä¸ªå¼•ç”¨éƒ½æ­£ç¡®å¯¹åº”',
        'æœ€ç»ˆï¼Œç»„ç»‡è¯­è¨€',
        'å¯èƒ½åˆ†ä¸ºå‡ ä¸ªä¸»è¦',
        'æˆ‘éœ€è¦å…ˆç¡®å®š',
        'ç„¶åï¼Œæ³¨æ„è¡Œå†…å¼•ç”¨',
        'éœ€è¦ç¡®ä¿è¯­è¨€æµç•…',
    ]
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # ğŸ”´ æ£€æŸ¥æ˜¯å¦æ˜¯æ€è€ƒè¿‡ç¨‹
        is_pure_thinking = any(indicator in line for indicator in thinking_indicators)
        is_meta_instruction = line.startswith(('ç”¨æˆ·', 'æ ¹æ®ç”¨æˆ·', 'ä¿¡æ¯1', 'ä¿¡æ¯2', 'ä¿¡æ¯3', 'ä¿¡æ¯4', 'ä¿¡æ¯5', 'ä¿¡æ¯6', 'å¼•ç”¨1', 'å¼•ç”¨2', 'å¼•ç”¨3'))
        is_task_analysis = 'ç»´åŸºç™¾ç§‘' in line and ('æ’°å†™' in line or 'ç« èŠ‚' in line or 'æ ¼å¼' in line)
        
        # ğŸ”´ è·³è¿‡æ˜æ˜¾çš„æ€è€ƒè¿‡ç¨‹ã€å…ƒæŒ‡ä»¤å’Œä»»åŠ¡åˆ†æ
        if is_pure_thinking or is_meta_instruction or is_task_analysis:
            continue
            
        useful_lines.append(line)
    
    # 6. ğŸ”´ å¦‚æœæ¸…ç†åæ²¡æœ‰å†…å®¹ï¼Œä½†åŸæ–‡åŒ…å«å¼•ç”¨ï¼Œé‡‡ç”¨æ›´ä¿å®ˆçš„æ¸…ç†ç­–ç•¥
    if not useful_lines and re.search(r'\[\d+\]', text):
        # æŒ‰æ®µè½é‡æ–°å¤„ç†
        paragraphs = text.split('\n\n')
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if re.search(r'\[\d+\]', paragraph):
                # åªç§»é™¤æ˜æ˜¾çš„æ€è€ƒå¼€å¤´å¥
                sentences = paragraph.split('ã€‚')
                keep_sentences = []
                for sentence in sentences:
                    sentence = sentence.strip()
                    if not sentence:
                        continue
                    # è·³è¿‡æ˜æ˜¾çš„æ€è€ƒå¥
                    skip_sentence = any(bad_start in sentence for bad_start in [
                        'å¥½çš„ï¼Œæˆ‘ç°åœ¨éœ€è¦', 'é¦–å…ˆï¼Œæˆ‘éœ€è¦', 'æ¥ä¸‹æ¥ï¼Œæˆ‘éœ€è¦', 'ç„¶åï¼Œéœ€è¦', 'æœ€åï¼Œè¦ç¡®ä¿',
                        'ç°åœ¨ï¼Œæˆ‘éœ€è¦', 'å¯èƒ½çš„ç»“æ„æ˜¯', 'éœ€è¦ç¡®ä¿', 'åœ¨ç»„ç»‡è¯­è¨€æ—¶', 'æœ€åï¼Œç¡®ä¿'
                    ])
                    if not skip_sentence:
                        keep_sentences.append(sentence)
                
                if keep_sentences:
                    reconstructed = 'ã€‚'.join(keep_sentences)
                    if reconstructed and not reconstructed.endswith('ã€‚'):
                        reconstructed += 'ã€‚'
                    useful_lines.append(reconstructed)
    
    text = '\n'.join(useful_lines)
    
    # 7. æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
    text = re.sub(r'\n\s*\n\s*\n', '\n\n', text)
    text = text.strip()
    
    return text


def process_dspy_output(prediction: Any) -> Any:
    """
    å¤„ç†DSPyé¢„æµ‹è¾“å‡ºï¼Œæ¸…ç†æ‰€æœ‰å­—ç¬¦ä¸²å­—æ®µ
    
    Args:
        prediction: DSPyé¢„æµ‹å¯¹è±¡
        
    Returns:
        å¤„ç†åçš„é¢„æµ‹å¯¹è±¡
    """
    if hasattr(prediction, '__dict__'):
        for key, value in prediction.__dict__.items():
            if isinstance(value, str):
                setattr(prediction, key, clean_chinese_output(value))
            elif isinstance(value, list):
                # å¤„ç†åˆ—è¡¨ä¸­çš„å­—ç¬¦ä¸²
                cleaned_list = []
                for item in value:
                    if isinstance(item, str):
                        cleaned_list.append(clean_chinese_output(item))
                    else:
                        cleaned_list.append(item)
                setattr(prediction, key, cleaned_list)
            elif isinstance(value, dict):
                # å¤„ç†å­—å…¸ä¸­çš„å­—ç¬¦ä¸²å€¼
                cleaned_dict = {}
                for k, v in value.items():
                    if isinstance(v, str):
                        cleaned_dict[k] = clean_chinese_output(v)
                    else:
                        cleaned_dict[k] = v
                setattr(prediction, key, cleaned_dict)
    
    return prediction


def wrap_chinese_predict(predict_class):
    """
    åŒ…è£…DSPy Predictç±»ï¼Œè‡ªåŠ¨å¤„ç†ä¸­æ–‡è¾“å‡º
    
    Args:
        predict_class: DSPy Predictç±»
        
    Returns:
        åŒ…è£…åçš„Predictç±»
    """
    original_forward = predict_class.forward
    
    def chinese_forward(self, **kwargs):
        # è°ƒç”¨åŸå§‹forwardæ–¹æ³•
        result = original_forward(self, **kwargs)
        
        # å¤„ç†è¾“å‡º
        return process_dspy_output(result)
    
    predict_class.forward = chinese_forward
    return predict_class


class ChinesePredict:
    """
    ä¸­æ–‡åŒ–çš„DSPy PredictåŒ…è£…å™¨
    """
    def __init__(self, signature, **kwargs):
        import dspy
        self.predict = dspy.Predict(signature, **kwargs)
    
    def __call__(self, **kwargs):
        result = self.predict(**kwargs)
        return process_dspy_output(result)
    
    def forward(self, **kwargs):
        result = self.predict.forward(**kwargs)
        return process_dspy_output(result)


# ============================================================================
# ä¸­æ–‡æ–‡æœ¬å¤„ç†å·¥å…· (åŸ chinese_text_utils.py)
# ============================================================================

def normalize_chinese_action_keywords(text: str) -> str:
    """æ ‡å‡†åŒ–ä¸­æ–‡åŠ¨ä½œå…³é”®è¯"""
    # åŠ¨ä½œå…³é”®è¯æ˜ å°„
    action_mappings = {
        "æ’å…¥": "insert",
        "è¿›å…¥": "step",
        "åˆ›å»º": "create",
        "æœ€ä½³æ”¾ç½®": "Best placement",
        "æ— åˆç†é€‰æ‹©": "No reasonable choice",
    }
    
    normalized_text = text
    for chinese, english in action_mappings.items():
        # ä¿æŒåŸæ–‡ï¼Œä½†ç¡®ä¿è§£æé€»è¾‘èƒ½è¯†åˆ«
        pass
    
    return normalized_text


def parse_chinese_node_expansion_output(output: str) -> List[str]:
    """è§£æä¸­æ–‡èŠ‚ç‚¹æ‰©å±•è¾“å‡º"""
    subsections = []
    for line in output.split("\n"):
        line = line.strip()
        if line:
            # ç§»é™¤å¯èƒ½çš„ç¼–å·å’Œç‰¹æ®Šå­—ç¬¦
            cleaned_line = re.sub(r'^[\d\.\s\-\*\+]*', '', line)
            cleaned_line = cleaned_line.strip()
            if cleaned_line:
                subsections.append(cleaned_line)
    
    return subsections


def clean_chinese_knowledge_structure(structure: str) -> str:
    """æ¸…ç†ä¸­æ–‡çŸ¥è¯†ç»“æ„æ˜¾ç¤º"""
    # æ ‡å‡†åŒ–å±‚çº§æ ‡è®°
    structure = re.sub(r'#+\s*', lambda m: '#' * len(m.group(0).strip()) + ' ', structure)
    return structure


def normalize_chinese_punctuation(text: str) -> str:
    """æ ‡å‡†åŒ–ä¸­è‹±æ–‡æ ‡ç‚¹ç¬¦å·"""
    # å°†è‹±æ–‡å†’å·æ›¿æ¢ä¸ºä¸­æ–‡å†’å·ï¼ˆåœ¨ä¸­æ–‡è¯­å¢ƒä¸­ï¼‰
    text = re.sub(r'(\w+):\s*([^\w])', r'\1ï¼š\2', text)
    
    # å¤„ç†å…¶ä»–æ ‡ç‚¹ç¬¦å·
    replacements = {
        'ï¼Œã€‚': 'ï¼Œã€‚',  # ä¿æŒä¸­æ–‡æ ‡ç‚¹
        'ï¼›ï¼': 'ï¼›ï¼',  # ä¿æŒä¸­æ–‡æ ‡ç‚¹
        'ï¼Ÿ': 'ï¼Ÿ',      # ä¿æŒä¸­æ–‡é—®å·
    }
    
    return text


def clean_chinese_expert_output(output: str) -> List[str]:
    """æ¸…ç†ä¸­æ–‡ä¸“å®¶è¾“å‡ºï¼Œæå–ä¸“å®¶è§’è‰²åˆ—è¡¨"""
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
    output = output.replace("*", "").replace("[", "").replace("]", "")
    
    expert_list = []
    for line in output.split("\n"):
        line = line.strip()
        # åŒ¹é…ä¸­æ–‡ç¼–å·æ ¼å¼ï¼š1. 2. æˆ– 1ã€2ã€
        patterns = [
            r"\d+\.\s*(.*)",  # è‹±æ–‡å¥å·
            r"\d+ã€\s*(.*)",  # ä¸­æ–‡é¡¿å·
            r"\d+ï¼š\s*(.*)",  # ä¸­æ–‡å†’å·
            r"\d+:\s*(.*)",   # è‹±æ–‡å†’å·
        ]
        
        for pattern in patterns:
            match = re.search(pattern, line)
            if match:
                expert_info = match.group(1).strip()
                if expert_info:
                    expert_list.append(expert_info)
                break
    
    return expert_list


def clean_chinese_section_text(text: str) -> str:
    """æ¸…ç†ä¸­æ–‡ç« èŠ‚æ–‡æœ¬ï¼ŒåŒ…æ‹¬thinkæ ‡ç­¾ã€å…ƒè®¤çŸ¥å†…å®¹å’Œæˆªæ–­å¤„ç†"""
    if not text:
        return text
    
    # ğŸ”´ é¦–å…ˆä½¿ç”¨é€šç”¨çš„ä¸­æ–‡è¾“å‡ºæ¸…ç†å‡½æ•°
    text = clean_chinese_output(text)
    
    # ç§»é™¤ä¸å®Œæ•´çš„å¥å­ï¼ˆé€šå¸¸ç”±äºè¾“å‡ºtokené™åˆ¶ï¼‰
    paragraphs = text.split("\n")
    
    # æ¸…ç†é€»è¾‘ï¼šå¦‚æœæ®µè½ä¸ä»¥ä¸­æ–‡æ ‡ç‚¹ç»“å°¾ï¼Œå¯èƒ½æ˜¯ä¸å®Œæ•´çš„
    chinese_end_punctuation = r'[ã€‚ï¼ï¼Ÿ]$'
    
    cleaned_paragraphs = []
    for paragraph in paragraphs:
        paragraph = paragraph.strip()
        if paragraph:
            # å¦‚æœæ˜¯æœ€åä¸€ä¸ªæ®µè½ä¸”æ²¡æœ‰ä»¥æ ‡ç‚¹ç»“å°¾ï¼Œå¯èƒ½æ˜¯ä¸å®Œæ•´çš„
            if paragraph == paragraphs[-1] and not re.search(chinese_end_punctuation, paragraph):
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å¼•ç”¨ï¼Œå¦‚æœæœ‰å¼•ç”¨è¯´æ˜å¯èƒ½æ˜¯å®Œæ•´çš„
                if re.search(r'\[\d+\]', paragraph):
                    cleaned_paragraphs.append(paragraph)
                # å¦åˆ™è·³è¿‡è¿™ä¸ªå¯èƒ½ä¸å®Œæ•´çš„æ®µè½
            else:
                cleaned_paragraphs.append(paragraph)
    
    return "\n".join(cleaned_paragraphs)


# ============================================================================
# é€šç”¨ä¸­æ–‡å¤„ç†å·¥å…·å‡½æ•°
# ============================================================================

def is_chinese_text(text: str) -> bool:
    """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦ä¸»è¦ä¸ºä¸­æ–‡"""
    if not text:
        return False
    
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    total_chars = len(re.findall(r'[^\s]', text))
    
    if total_chars == 0:
        return False
    
    return chinese_chars / total_chars > 0.5


def extract_chinese_sentences(text: str) -> List[str]:
    """æå–ä¸­æ–‡å¥å­"""
    # æŒ‰ä¸­æ–‡æ ‡ç‚¹ç¬¦å·åˆ†å‰²å¥å­
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›]', text)
    
    # æ¸…ç†ç©ºå¥å­å’Œè¿‡çŸ­çš„å¥å­
    chinese_sentences = []
    for sentence in sentences:
        sentence = sentence.strip()
        if sentence and len(sentence) > 3 and is_chinese_text(sentence):
            chinese_sentences.append(sentence)
    
    return chinese_sentences


def format_chinese_list(items: List[str], style: str = "numbered") -> str:
    """æ ¼å¼åŒ–ä¸­æ–‡åˆ—è¡¨
    
    Args:
        items: åˆ—è¡¨é¡¹
        style: æ ¼å¼æ ·å¼ ("numbered", "bullet", "chinese_number")
    """
    if not items:
        return ""
    
    formatted_items = []
    
    for i, item in enumerate(items, 1):
        if style == "numbered":
            formatted_items.append(f"{i}. {item}")
        elif style == "bullet":
            formatted_items.append(f"â€¢ {item}")
        elif style == "chinese_number":
            chinese_numbers = ["ä¸€", "äºŒ", "ä¸‰", "å››", "äº”", "å…­", "ä¸ƒ", "å…«", "ä¹", "å"]
            if i <= len(chinese_numbers):
                formatted_items.append(f"{chinese_numbers[i-1]}ã€{item}")
            else:
                formatted_items.append(f"{i}ã€{item}")
        else:
            formatted_items.append(item)
    
    return "\n".join(formatted_items)


def clean_mixed_language_text(text: str, prefer_chinese: bool = True) -> str:
    """æ¸…ç†æ··åˆè¯­è¨€æ–‡æœ¬ï¼Œä¼˜å…ˆä¿ç•™æŒ‡å®šè¯­è¨€
    
    Args:
        text: è¾“å…¥æ–‡æœ¬
        prefer_chinese: æ˜¯å¦ä¼˜å…ˆä¿ç•™ä¸­æ–‡
    """
    if not text:
        return text
    
    # å…ˆè¿›è¡ŒåŸºæœ¬æ¸…ç†
    text = clean_chinese_output(text)
    
    if prefer_chinese:
        # æå–ä¸­æ–‡å¥å­
        chinese_sentences = extract_chinese_sentences(text)
        if chinese_sentences:
            return "ã€‚".join(chinese_sentences) + "ã€‚"
    
    return text


# ============================================================================
# ä¾¿æ·å‡½æ•°
# ============================================================================

def process_all_chinese_text(text_dict: Dict[str, str]) -> Dict[str, str]:
    """æ‰¹é‡å¤„ç†å­—å…¸ä¸­çš„æ‰€æœ‰ä¸­æ–‡æ–‡æœ¬"""
    processed = {}
    for key, value in text_dict.items():
        if isinstance(value, str):
            processed[key] = clean_chinese_output(value)
        else:
            processed[key] = value
    return processed


def get_text_statistics(text: str) -> Dict[str, int]:
    """è·å–æ–‡æœ¬ç»Ÿè®¡ä¿¡æ¯"""
    return {
        "æ€»å­—ç¬¦æ•°": len(text),
        "ä¸­æ–‡å­—ç¬¦æ•°": len(re.findall(r'[\u4e00-\u9fff]', text)),
        "è‹±æ–‡å•è¯æ•°": len(re.findall(r'\b[a-zA-Z]+\b', text)),
        "æ•°å­—æ•°é‡": len(re.findall(r'\d+', text)),
        "æ ‡ç‚¹ç¬¦å·æ•°": len(re.findall(r'[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹]', text)),
        "å¥å­æ•°": len(extract_chinese_sentences(text)),
    } 