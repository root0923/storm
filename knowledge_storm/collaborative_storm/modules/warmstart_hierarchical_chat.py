"""
Warm starts the Co-STORM system by conducting a background information search to establish a shared conceptual space with the user.
 
This stage functions as a mini-STORM, where multiple LLM agents are spawned with different perspectives to engage in multi-round conversations. 
The knowledge base (represented as a mind map) is initialized using the information gathered during these exchanges.

Additionally, the system generates a first draft of the report, which is then used to create a concise and engaging conversation. 
The synthesized conversation is presented to the user to help them quickly catch up on the system's current knowledge about the topic.
"""

import dspy
import concurrent.futures
from threading import Lock
from typing import List, Optional, Union, TYPE_CHECKING

from .callback import BaseCallbackHandler
from .collaborative_storm_utils import _get_answer_question_module_instance
from .expert_generation import GenerateExpertModule
from .grounded_question_answering import AnswerQuestionModule
from .chinese_utils import clean_chinese_output
from ...dataclass import ConversationTurn, KnowledgeBase, KnowledgeNode
from ...interface import LMConfigs
from ...logging_wrapper import LoggingWrapper
from ...storm_wiki.modules.outline_generation import WritePageOutline
from ...utils import ArticleTextProcessing as AP
import re


if TYPE_CHECKING:
    from ..engine import RunnerArgument


class WarmStartModerator(dspy.Signature):
    """
    æ‚¨æ˜¯åœ†æ¡Œè®¨è®ºçš„ä¸»æŒäººã€‚ç›®æ ‡æ˜¯ä¸å¤šä½ä¸“å®¶èŠå¤©ï¼Œè®¨è®ºä¸»é¢˜çš„äº‹å®å’ŒèƒŒæ™¯ï¼Œè®©è§‚ä¼—ç†Ÿæ‚‰è¯¥ä¸»é¢˜ã€‚
    æ‚¨å°†çœ‹åˆ°ä¸»é¢˜ã€æ‚¨å·²ç»é—®è¿‡çš„é—®é¢˜å†å²ï¼Œä»¥åŠæ‚¨æ­£åœ¨è®¨è®ºçš„å½“å‰ä¸“å®¶ã€‚
    åŸºäºè¿™äº›ä¿¡æ¯ï¼Œä¸ºå½“å‰ä¸“å®¶ç”Ÿæˆä¸‹ä¸€ä¸ªé—®é¢˜ä»¥æ¨è¿›è®¨è®ºã€‚

    è¾“å‡ºåº”è¯¥åªåŒ…å«ç»™å½“å‰ä¸“å®¶çš„ä¸‹ä¸€ä¸ªé—®é¢˜ã€‚ä¸è¦åŒ…å«ä»»ä½•å…¶ä»–ä¿¡æ¯æˆ–å‰è¨€ã€‚
    """

    topic = dspy.InputField(prefix="åœ†æ¡Œè®¨è®ºçš„ä¸»é¢˜ï¼š", format=str)
    history = dspy.InputField(
        prefix="æ‚¨å·²ç»äº¤æµè¿‡çš„ä¸“å®¶ï¼š", format=str
    )
    current_expert = dspy.InputField(prefix="æ‚¨æ­£åœ¨äº¤æµçš„ä¸“å®¶ï¼š", format=str)
    question = dspy.OutputField(
        prefix="ç»™æ‚¨æ­£åœ¨äº¤æµçš„ä¸“å®¶çš„ä¸‹ä¸€ä¸ªé—®é¢˜ï¼š", format=str
    )


class SectionToConvTranscript(dspy.Signature):
    """
    ç»™æ‚¨ä¸€ä¸ªå…³äºç‰¹å®šä¸»é¢˜çš„ç®€è¦æŠ¥å‘Šç« èŠ‚ã€‚æ‚¨çš„ä»»åŠ¡æ˜¯å°†è¿™ä¸ªç« èŠ‚è½¬æ¢ä¸ºåœ†æ¡Œå¯¹è¯çš„å¼•äººå…¥èƒœçš„å¼€åœºè®¨è®ºã€‚
    ç›®æ ‡æ˜¯å¸®åŠ©å‚ä¸è€…å’Œè§‚ä¼—å¿«é€Ÿç†è§£å…³é”®ä¿¡æ¯ã€‚
    é—®é¢˜å’Œç­”æ¡ˆéƒ½åº”è¯¥ç”¨é¢å‘è§‚ä¼—çš„åœ†æ¡Œè®¨è®ºè¯­è°ƒã€‚

    å…·ä½“è€Œè¨€ï¼Œæ‚¨éœ€è¦ï¼š
    1. ç”Ÿæˆä¸€ä¸ªå¼•äººå…¥èƒœçš„é—®é¢˜ï¼Œåˆ©ç”¨ç« èŠ‚åç§°å’Œä¸»é¢˜æ¥å¼€å¯å†…å®¹è®¨è®ºã€‚
    2. æä¾›ä¸€ä¸ªç®€æ´è€Œå¼•äººå…¥èƒœçš„ç­”æ¡ˆï¼ˆåŒ…å«åŸæ–‡çš„æ‰€æœ‰è¡Œå†…å¼•ç”¨ï¼‰ï¼Œæºäºè¯¥ç« èŠ‚ï¼Œä½œä¸ºæŒ‡å¼•ï¼Œé¿å…è¿‡å¤šç»†èŠ‚ã€‚
    
    ã€ä¸¥æ ¼ç¦æ­¢ã€‘ï¼š
    - ä¸å¾—å‡ºç°<think>æ ‡ç­¾æˆ–è‹±æ–‡æ€è€ƒå†…å®¹
    - ä¸å¾—è¾“å‡ºå†…éƒ¨æ€è€ƒè¿‡ç¨‹
    - ç›´æ¥ç»™å‡ºé—®é¢˜å’Œç­”æ¡ˆå†…å®¹
    """

    topic = dspy.InputField(prefix="ä¸»é¢˜ï¼š", format=str)
    section_name = dspy.InputField(prefix="ç« èŠ‚åç§°ï¼š", format=str)
    section_content = dspy.InputField(prefix="ç« èŠ‚å†…å®¹ï¼š", format=str)
    question = dspy.OutputField(prefix="ç°åœ¨åªç»™å‡ºå¼•äººå…¥èƒœçš„é—®é¢˜ã€‚\né—®é¢˜ï¼š")
    answer = dspy.OutputField(
        prefix="ç°åœ¨åªç»™å‡ºå¼•äººå…¥èƒœçš„ç­”æ¡ˆï¼ŒåŒ…å«åŸæ–‡çš„æ‰€æœ‰è¡Œå†…å¼•ç”¨ã€‚\nç­”æ¡ˆï¼š"
    )


class ReportToConversation(dspy.Module):
    def __init__(self, engine: Union[dspy.dsp.LM, dspy.dsp.HFModel]):
        self.engine = engine
        self.section_to_conv_transcript = dspy.Predict(SectionToConvTranscript)

    def forward(self, knowledge_base: KnowledgeBase):
        def process_node(node, topic):
            with dspy.settings.context(lm=self.engine, show_guidelines=False):
                output = self.section_to_conv_transcript(
                    topic=topic,
                    section_name=node.get_path_from_root(),
                    section_content=node.synthesize_output,
                )
                # ğŸ”´ ç«‹å³æ¸…ç†LLMè¾“å‡ºä¸­çš„thinkæ ‡ç­¾
                question = clean_chinese_output(output.question.replace("é—®é¢˜ï¼š", "").strip())
                answer = clean_chinese_output(output.answer.replace("ç­”æ¡ˆï¼š", "").strip())
                return question, answer

        conversations = []
        nodes = knowledge_base.collect_all_nodes()
        nodes = [node for node in nodes if node.name != "root" and node.content]
        topic = knowledge_base.topic

        with concurrent.futures.ThreadPoolExecutor() as executor:
            future_to_node = {
                executor.submit(process_node, node, topic): node for node in nodes
            }
            for future in concurrent.futures.as_completed(future_to_node):
                node = future_to_node[future]
                question, answer = future.result()
                
                conversations.append(
                    ConversationTurn(
                        role="èƒŒæ™¯è®¨è®ºä¸»æŒäºº",
                        raw_utterance=question,
                        utterance_type="Original Question",
                        utterance=question,
                        cited_info=[
                            knowledge_base.info_uuid_to_info_dict[idx]
                            for idx in AP.parse_citation_indices(question)
                        ],
                    )
                )
                conversations.append(
                    ConversationTurn(
                        role="èƒŒæ™¯è®¨è®ºä¸“å®¶",
                        raw_utterance=answer,
                        utterance_type="Potential Answer",
                        utterance=answer,
                        cited_info=[
                            knowledge_base.info_uuid_to_info_dict[idx]
                            for idx in AP.parse_citation_indices(answer)
                        ],
                    )
                )
        return conversations


class WarmStartConversation(dspy.Module):
    def __init__(
        self,
        question_asking_lm: Union[dspy.dsp.LM, dspy.dsp.HFModel],
        generate_expert_module: GenerateExpertModule,
        answer_question_module: AnswerQuestionModule,
        logging_wrapper: LoggingWrapper,
        max_num_experts: int = 3,
        max_turn_per_experts: int = 2,
        max_thread: int = 3,
        callback_handler: BaseCallbackHandler = None,
    ):
        self.ask_question = dspy.Predict(WarmStartModerator)
        self.max_num_experts = max_num_experts
        self.max_turn_per_experts = max_turn_per_experts
        self.question_asking_lm = question_asking_lm
        self.answer_question_module = answer_question_module
        self.max_thread = max_thread
        self.generate_experts_module = generate_expert_module
        self.logging_wrapper = logging_wrapper
        self.callback_handler = callback_handler

    def format_dialogue_question_history_string(
        self, conversation_history: List[ConversationTurn]
    ):
        output = []
        for idx, turn in enumerate(conversation_history):
            info = turn.claim_to_make if turn.claim_to_make else turn.utterance
            output.append(f"{idx + 1}: {info}")
        return "\n".join(output)

    def generate_warmstart_experts(self, topic: str):
        background_seeking_dialogue = self.get_background_info(topic=topic)
        background_info = background_seeking_dialogue.utterance
        gen_expert_output = self.generate_experts_module(
            topic=topic,
            background_info=background_info,
            num_experts=self.max_num_experts,
        )
        return gen_expert_output.experts, background_seeking_dialogue

    def get_background_info(self, topic: str):
        question = f"å…³äº{topic}çš„èƒŒæ™¯ä¿¡æ¯"
    
        answer = self.answer_question_module(
            topic=topic, question=question, mode="extensive", style="å¯¹è¯æ€§"
        )

        # ğŸŸ¢ æ¸…ç†thinkæ ‡ç­¾å’Œè‹±æ–‡æ€è€ƒå†…å®¹
        cleaned_response = clean_chinese_output(answer.response)

        return ConversationTurn(
            role="èƒŒæ™¯ä¿¡æ¯ç ”ç©¶å‘˜",
            raw_utterance=cleaned_response,
            utterance_type="Questioning",
            claim_to_make=question,
            queries=answer.queries,
            raw_retrieved_info=answer.raw_retrieved_info,
            cited_info=answer.cited_info,
        )

    def forward(self, topic: str):
        with self.logging_wrapper.log_event(
            "warm start, perspective guided QA: identify experts"
        ):
            # do background research, generate some experts
            experts, background_seeking_dialogue = self.generate_warmstart_experts(
                topic=topic
            )
        # init list to store the dialogue history
        conversation_history: List[ConversationTurn] = []
        lock = Lock()

        # hierarchical chat: chat with one expert. Generate question, get answer
        def process_expert(expert):
            # æ”¯æŒä¸­è‹±æ–‡å†’å·åˆ†å‰²
            colon_pattern = r'[ï¼š:]'
            if re.search(colon_pattern, expert):
                parts = re.split(colon_pattern, expert, 1)
                expert_name = parts[0].strip()
                expert_description = parts[1].strip() if len(parts) > 1 else ""
            else:
                expert_name = expert.strip()
                expert_description = ""
            for idx in range(self.max_turn_per_experts):
                with self.logging_wrapper.log_event(
                    f"warm start, perspective guided QA: expert {expert_name}; turn {idx + 1}"
                ):
                    try:
                        with lock:
                            history = self.format_dialogue_question_history_string(
                                conversation_history
                            )
                        with dspy.settings.context(lm=self.question_asking_lm):
                            question = self.ask_question(
                                topic=topic, history=history, current_expert=expert
                            ).question
                            # ğŸŸ¢ æ¸…ç†thinkæ ‡ç­¾å’Œè‹±æ–‡æ€è€ƒå†…å®¹
                            question = clean_chinese_output(question)
                        answer = self.answer_question_module(
                            topic=topic,
                            question=question,
                            mode="brief",
                            style="å¯¹è¯æ€§",
                        )
                        
                        # ğŸŸ¢ æ¸…ç†thinkæ ‡ç­¾å’Œè‹±æ–‡æ€è€ƒå†…å®¹
                        cleaned_response = clean_chinese_output(answer.response)
                        
                        conversation_turn = ConversationTurn(
                            role=expert,
                            claim_to_make=question,
                            raw_utterance=cleaned_response,
                            utterance_type="Support",
                            queries=answer.queries,
                            raw_retrieved_info=answer.raw_retrieved_info,
                            cited_info=answer.cited_info,
                        )
                        if self.callback_handler is not None:
                            self.callback_handler.on_warmstart_update(
                                message="\n".join(
                                    [
                                        f"Finish browsing {url}"
                                        for url in [
                                            i.url for i in answer.raw_retrieved_info
                                        ]
                                    ]
                                )
                            )
                        with lock:
                            conversation_history.append(conversation_turn)
                    except Exception as e:
                        print(f"Error processing expert {expert}: {e}")

        # multi-thread conversation
        with concurrent.futures.ThreadPoolExecutor(
            max_workers=self.max_thread
        ) as executor:
            futures = [
                executor.submit(process_expert, expert)
                for expert in experts[: min(len(experts), self.max_num_experts)]
            ]
            concurrent.futures.wait(futures)

        conversation_history = [background_seeking_dialogue] + conversation_history

        return dspy.Prediction(
            conversation_history=conversation_history, experts=experts
        )


class GenerateWarmStartOutline(dspy.Signature):
    """æ ¹æ®åœ†æ¡Œè®¨è®ºç”Ÿæˆç±»ç»´åŸºç™¾ç§‘æŠ¥å‘Šçš„å¤§çº²ã€‚æ‚¨å°†çœ‹åˆ°å¯¹è¯ä¸­çš„è®¨è®ºè¦ç‚¹å’Œç›¸åº”æŸ¥è¯¢ã€‚
    æ‚¨å°†è·å¾—ä¸€ä¸ªè‰æ¡ˆå¤§çº²ï¼Œå¯ä»¥ä»ä¸­è·å¾—ä¸€äº›çµæ„Ÿã€‚ä¸è¦åŒ…å«ç»™å®šè®¨è®ºå†å²ä¸­æœªæåŠçš„ç« èŠ‚ã€‚
    
    é‡è¦è¯´æ˜ï¼š
    - è®¨è®ºå†å²æ˜¯ä»¥"é—®é¢˜ï¼š"å¼€å¤´çš„å¯¹è¯è®°å½•ï¼Œä¸æ˜¯ç« èŠ‚æ ‡é¢˜
    - æ‚¨éœ€è¦æ ¹æ®è¿™äº›è®¨è®ºå†…å®¹æ¥ç”Ÿæˆç®€æ´çš„ç« èŠ‚æ ‡é¢˜
    - ç« èŠ‚æ ‡é¢˜åº”è¯¥ç®€æ´æ˜äº†ï¼Œå¦‚"æ¦‚è¿°"ã€"æŠ€æœ¯æ¶æ„"ã€"åº”ç”¨åœºæ™¯"ç­‰
    
    æ ¼å¼è¦æ±‚ï¼š
    1. ä½¿ç”¨"#"è¡¨ç¤ºç« èŠ‚æ ‡é¢˜ï¼Œ"##"è¡¨ç¤ºå­ç« èŠ‚æ ‡é¢˜ï¼Œ"###"è¡¨ç¤ºå­å­ç« èŠ‚æ ‡é¢˜ï¼Œä¾æ­¤ç±»æ¨ã€‚
    2. ä¸è¦åŒ…å«ä»»ä½•é™„åŠ ä¿¡æ¯æˆ–è§£é‡Šæ€§æ–‡å­—ã€‚
    3. ä»å¤§çº²ä¸­æ’é™¤ä¸»é¢˜åç§°ã€‚
    4. ç« èŠ‚æ ‡é¢˜è¦ç®€æ´ï¼Œé¿å…å†—é•¿çš„æè¿°æ€§è¯­å¥ã€‚
    å¤§çº²çš„ç»„ç»‡åº”é‡‡ç”¨ç»´åŸºç™¾ç§‘é£æ ¼ã€‚
    """

    topic = dspy.InputField(prefix="è®¨è®ºçš„ä¸»é¢˜ï¼š", format=str)
    draft = dspy.InputField(prefix="æ‚¨å¯ä»¥å‚è€ƒçš„è‰æ¡ˆå¤§çº²ï¼š", format=str)
    conv = dspy.InputField(prefix="è®¨è®ºå†å²ï¼š\n", format=str)
    outline = dspy.OutputField(
        prefix='è¯·ç”Ÿæˆç®€æ´çš„ç»´åŸºç™¾ç§‘é£æ ¼å¤§çº²ï¼ˆä½¿ç”¨"# æ ‡é¢˜"è¡¨ç¤ºç« èŠ‚æ ‡é¢˜ï¼Œ"## æ ‡é¢˜"è¡¨ç¤ºå­ç« èŠ‚æ ‡é¢˜...ï¼‰ï¼š\n',
        format=str,
    )


class GenerateWarmStartOutlineModule(dspy.Module):
    def __init__(self, engine: Union[dspy.dsp.LM, dspy.dsp.HFModel]):
        self.engine = engine
        self.gen_outline = dspy.Predict(GenerateWarmStartOutline)
        self.draft_outline = dspy.Predict(WritePageOutline)

    def extract_questions_and_queries(self, conv: List[ConversationTurn]):
        context = []
        for turn in conv:
            focus = turn.claim_to_make
            queries = turn.queries
            queries_string = "\n".join(
                f"  - {query}" for query in queries  # ä½¿ç”¨æ›´æ¸…æ™°çš„æ ¼å¼
            )
            string = f"é—®é¢˜ï¼š{focus}\nç›¸å…³æŸ¥è¯¢ï¼š\n{queries_string}"  # ç®€åŒ–æ ¼å¼
            context.append(string)
        return "\n\n".join(context)  # ç”¨åŒæ¢è¡Œåˆ†éš”ä¸åŒçš„é—®é¢˜

    def get_draft_outline(self, topic: str):
        with dspy.settings.context(lm=self.engine):
            return self.draft_outline(topic=topic).outline

    def forward(self, topic: str, conv: List[ConversationTurn]):
        discussion_history = self.extract_questions_and_queries(conv)
        draft_outline = self.get_draft_outline(topic=topic)
        # ğŸŸ¢ æ¸…ç†thinkæ ‡ç­¾å’Œè‹±æ–‡æ€è€ƒå†…å®¹
        draft_outline = clean_chinese_output(draft_outline)
        with dspy.settings.context(lm=self.engine):
            outline = self.gen_outline(
                topic=topic, draft=draft_outline, conv=discussion_history
            ).outline
            outline = AP.clean_up_outline(outline)
        return dspy.Prediction(outline=outline, draft_outline=draft_outline)


class WarmStartModule:
    def __init__(
        self,
        lm_config: LMConfigs,
        runner_argument: "RunnerArgument",
        logging_wrapper: LoggingWrapper,
        rm: Optional[dspy.Retrieve] = None,
        callback_handler: BaseCallbackHandler = None,
    ):
        generate_expert_module = GenerateExpertModule(
            engine=lm_config.discourse_manage_lm
        )
        self.warmstart_conv = WarmStartConversation(
            question_asking_lm=lm_config.question_asking_lm,
            generate_expert_module=generate_expert_module,
            answer_question_module=_get_answer_question_module_instance(
                lm_config=lm_config,
                runner_argument=runner_argument,
                logging_wrapper=logging_wrapper,
                rm=rm,
                deepsearcher_api_url=runner_argument.deepsearcher_api_url,
            ),
            max_num_experts=runner_argument.warmstart_max_num_experts,
            max_turn_per_experts=runner_argument.warmstart_max_turn_per_experts,
            max_thread=runner_argument.warmstart_max_thread,
            logging_wrapper=logging_wrapper,
            callback_handler=callback_handler,
        )
        self.warmstart_outline_gen_module = GenerateWarmStartOutlineModule(
            engine=lm_config.warmstart_outline_gen_lm
        )
        self.report_to_conversation = ReportToConversation(lm_config.knowledge_base_lm)
        self.logging_wrapper = logging_wrapper
        self.callback_handler = callback_handler

    def initiate_warm_start(self, topic: str, knowledge_base: KnowledgeBase):
        """
        Initiates a warm start process for the given topic by generating a warm start conversation and inserting the
        resulting information into a knowledge base.

        Args:
            topic (str): The topic for which to initiate the warm start process.

        Returns:
            Tuple[List[ConversationTurn], List[str], KnowledgeBase]:
                - A list of ConversationTurn instances representing the conversation history.
                - A list of strings representing the experts involved in the conversation.
                - A KnowledgeBase instance containing the organized information.
        """
        warm_start_conversation_history: List[ConversationTurn] = []
        warm_start_experts = None
        # get warm start conversations
        with self.logging_wrapper.log_event("warm start: perspective guided QA"):
            if self.callback_handler is not None:
                self.callback_handler.on_warmstart_update(
                    message="Start getting familiar with the topic by chatting with multiple LLM experts (Step 1 / 4)"
                )
            warm_start_result = self.warmstart_conv(topic=topic)
            warm_start_conversation_history = warm_start_result.conversation_history
            warm_start_experts = warm_start_result.experts

        # get warm start conv outline
        with self.logging_wrapper.log_event("warm start: outline generation"):
            if self.callback_handler is not None:
                self.callback_handler.on_warmstart_update(
                    "Organizing collected information (Step 2 / 4)"
                )
            warm_start_outline_output = self.warmstart_outline_gen_module(
                topic=topic, conv=warm_start_conversation_history
            )
        # init knowledge base
        with self.logging_wrapper.log_event("warm start: insert into knowledge base"):
            if self.callback_handler is not None:
                self.callback_handler.on_warmstart_update(
                    "Inserting collected information into knowledge base (Step 3 / 4)"
                )
            knowledge_base.insert_from_outline_string(
                outline_string=warm_start_outline_output.outline
            )
            # insert information to knowledge base
            for turn in warm_start_conversation_history:
                knowledge_base.update_from_conv_turn(
                    conv_turn=turn, allow_create_new_node=False
                )
        # knowledge base to report
        if self.callback_handler is not None:
            self.callback_handler.on_warmstart_update(
                "Synthesizing background information discussion utterances (Step 4 / 4)"
            )
        knowledge_base.to_report()

        # generate engaging conversations
        engaging_conversations = self.report_to_conversation(knowledge_base)
        return (
            warm_start_conversation_history,
            engaging_conversations,
            warm_start_experts,
        )
